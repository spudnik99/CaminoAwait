{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Camino de Santiago__ \n",
    "## Pilgrimage Data Retrieveal from the Cathedral RESTful Server \n",
    "### __Purpose:__ To extract data for storage in Excel and CSV files for analysis\n",
    "#### *Notebook uses __serial async calls__ to avoid DoS defensive timeouts by the Server when concurrent gets are issued*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 1:__ Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time            ### To work with time objects\n",
    "import json            ### To Process returned JSON data\n",
    "import pandas as pd    ### To work with dataframes\n",
    "import numpy as np     ### To work with dataframes\n",
    "import asyncio         ### To make async request\n",
    "import httpx           ### To make request to Cathedral URL\n",
    "import datetime        ### To work with datetime\n",
    "import openpyxl        ### To save to Excel files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 2:__ Initialize Panda dataframes to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Dataframes to fill\n",
    " \n",
    "tablelist = [\n",
    "    'Anho',\n",
    "    'Mes',\n",
    "    'Identificador',\n",
    "    'TotalRegistros',\n",
    "    'Autonomias',\n",
    "    'Caminos',\n",
    "    'Continentes',\n",
    "    'Edades',\n",
    "    'Grupos',\n",
    "    'Medios',\n",
    "    'Motivos',\n",
    "    'Paises',\n",
    "    'Procedencias',\n",
    "    'Sexos'\n",
    "]\n",
    "totals_df = pd.DataFrame(\n",
    "    columns=['Anho', 'Mes', 'Identificador', 'TotalRegistros'])\n",
    "autonomous_coms_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "routes_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "continents_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "ages_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "groups_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "transportation_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "motives_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "countries_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "origin_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n",
    "gender_df = pd.DataFrame(\n",
    "    columns=['id', 'Anho', 'Mes', 'Nombre', 'Total', 'Porcentaje'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 3:__ Define functions that calculate the valid range of dates that may be retrieved from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Function that calculates the range of valid months\n",
    "def range_of_months(start_date, end_date):\n",
    "    months = []\n",
    "\n",
    "    for i in range(start_date.year * 12 + start_date.month, end_date.year*12+end_date.month):\n",
    "        #months.append(datetime.date((i-13) // 12 + 1, (i-1) % 12 + 1, 1))\n",
    "        date_iter = datetime.date((i-13) // 12 + 1, (i-1) % 12 + 1, 1)\n",
    "        year = date_iter.year\n",
    "        month = date_iter.month\n",
    "        months.append([year, month])\n",
    "    return months\n",
    "\n",
    "\n",
    "data_start_date = datetime.date(2004, 1, 1) #start date is January 2004\n",
    "data_end_date = datetime.datetime.now()  # datetime.date(2011, 2, 1)\n",
    "year_months = []\n",
    "year_months = range_of_months(data_start_date, data_end_date)\n",
    "#print(year_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 4:__ Define functions that concatenate newly retrieved data to previously retrieved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(idx, yr, mnth, df):\n",
    "    df.insert(0, \"id\", idx)\n",
    "    df.insert(1, \"Anho\", yr)\n",
    "    df.insert(2, \"Mes\", mnth)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_to_dataframe(df_running_totals, yr, mnth, df_to_add):\n",
    "    #print('dataframe1:',df1)\n",
    "    #print('dataframe2:', df2)\n",
    "    idx = str(yr)+str(mnth).zfill(2)\n",
    "    add_prefix(idx, yr, mnth, df_to_add)\n",
    "    frames = [df_running_totals, df_to_add]\n",
    "    tempdf = pd.concat(frames, ignore_index=True)\n",
    "    #print('temp',tempdf)\n",
    "\n",
    "    return tempdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 5:__ Defines function that splits and converts retrieved json data to various the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Async function that requests data from the Cathedral's server ################\n",
    "############# waits for response from server before it continues with requests to avoid empty responses (a \"denial of service\" defense mechanism? )\n",
    "\n",
    "\n",
    "async def get_data(mes, anho):\n",
    "    eMes = str(mes)\n",
    "    eAnho = str(anho)\n",
    "    url = 'https://catedral.df-server.info/ws/wsCatedral.asmx/ObtenerEstadisticasMes?eAnho=' + eAnho + '&eMes=' + eMes\n",
    "    # print(\"mes:\",mes,\"anho:\",anho,\"url:\",url)\n",
    "    client = httpx.AsyncClient()\n",
    "    async with client.stream('GET', url) as response:\n",
    "        async for chunk in response.aiter_text():\n",
    "           # print(\"Chunk:\",chunk)\n",
    "            if len(chunk) > 0:\n",
    "                json_data_chunk = json.loads(chunk)\n",
    "                totals_row = [anho, mes, json_data_chunk['Identificador'],json_data_chunk['TotalRegistros']]\n",
    "                global totals_df\n",
    "                totals_df.loc[len(totals_df.index)] = totals_row\n",
    "\n",
    "                # Concatenate new results and running total dataframes, split JSON by key\n",
    "                global autonomous_coms_df\n",
    "                autonomous_coms_df=add_to_dataframe(autonomous_coms_df, anho, mes, pd.DataFrame(json_data_chunk['Autonomias']))\n",
    "                global routes_df\n",
    "                routes_df=add_to_dataframe(routes_df, anho, mes, pd.DataFrame(json_data_chunk['Caminos']))\n",
    "                global continents_df\n",
    "                continents_df=add_to_dataframe(continents_df, anho, mes, pd.DataFrame(json_data_chunk['Continentes']))\n",
    "                global ages_df\n",
    "                ages_df=add_to_dataframe(ages_df, anho, mes, pd.DataFrame(json_data_chunk['Edades']))\n",
    "                global groups_df\n",
    "                groups_df=add_to_dataframe(groups_df, anho, mes, pd.DataFrame(json_data_chunk['Grupos']))\n",
    "                global transportation_df\n",
    "                transportation_df=add_to_dataframe(transportation_df, anho, mes, pd.DataFrame(json_data_chunk['Medios']))\n",
    "                global motives_df\n",
    "                motives_df=add_to_dataframe(motives_df, anho, mes, pd.DataFrame(json_data_chunk['Motivos']))\n",
    "                global countries_df\n",
    "                countries_df=add_to_dataframe(countries_df, anho, mes, pd.DataFrame(json_data_chunk['Paises']))\n",
    "                global origin_df\n",
    "                origin_df=add_to_dataframe(origin_df, anho, mes, pd.DataFrame(json_data_chunk['Procedencias']))\n",
    "                global gender_df\n",
    "                gender_df=add_to_dataframe(gender_df, anho, mes, pd.DataFrame(json_data_chunk['Sexos']))\n",
    "\n",
    "                print('Mes:', eMes, 'Anho:', eAnho)\n",
    "                #print(\"start:\", json_data_chunk, \":end\", len(chunk))\n",
    "            else:\n",
    "                break\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    #    response = await client.stream('GET',url)\n",
    "    #    print(response)\n",
    "\n",
    "    ######### TEST ###############\n",
    "    \"\"\" \n",
    "    for yAnho in range(2004,2005):\n",
    "        for xMes in range(1,12):\n",
    "            await get_data(xMes,yAnho)\n",
    "    \"\"\"    \n",
    "    ######### TEST END ###########  \n",
    "\n",
    "\n",
    "data_start_date = datetime.date(2004, 1, 1)\n",
    "#data_end_date = datetime.date(2004, 3, 1)\n",
    "data_end_date = datetime.datetime.now()  # datetime.date(2011, 2, 1)\n",
    "year_months = []\n",
    "year_months = range_of_months(data_start_date, data_end_date)\n",
    "#print(year_months)\n",
    "\n",
    "\n",
    "for year_month in range(len(year_months)):\n",
    "    await get_data(year_months[year_month][1],year_months[year_month][0])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 6:__ Rename columns to proper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {\n",
    "    \"id\": \"ID\",  \"anho\": \"Anho\", \"mes\": \"Mes\", \"Nombre\": \"Descripcion\", \"Total\": \"Total\",  \"Porcentaje\": \"Porcentaje\"}\n",
    "autonomous_coms_df.rename(columns=column_names, inplace=True)\n",
    "\n",
    "routes_df.rename(columns=column_names, inplace=True)\n",
    "continents_df.rename(columns=column_names, inplace=True)\n",
    "ages_df.rename(columns=column_names, inplace=True)\n",
    "groups_df.rename(columns=column_names, inplace=True)\n",
    "transportation_df.rename(columns=column_names, inplace=True)\n",
    "motives_df.rename(columns=column_names, inplace=True)\n",
    "countries_df.rename(columns=column_names, inplace=True)\n",
    "origin_df.rename(columns=column_names, inplace=True)\n",
    "gender_df.rename(columns=column_names, inplace=True)\n",
    "\n",
    "### Clean up some name changes\n",
    "origin_df['Descripcion'] = np.where(origin_df['Descripcion'] == \"Vilafranca\", \"Villafranca del Bierzo\", origin_df['Descripcion'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 7:__ Totals JSON from the server has an error, use genders to generate a true total using Pandas groupby with the sum() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_true_df=gender_df.groupby(['ID','Anho','Mes'], as_index =False)['Total'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 8:__ Create __\"wide\"__ tables from  __\"long\"__ data tables, in this step all nulls are converted to zero because these are numeric counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "autonomous_coms_df2=autonomous_coms_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "autonomous_coms_wide_df=pd.pivot(autonomous_coms_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = autonomous_coms_df['Descripcion'].unique()\n",
    "autonomous_coms_wide_df=autonomous_coms_wide_df[cols]\n",
    "autonomous_coms_wide_df.fillna(0,inplace=True)\n",
    "autonomous_coms_wide_df['Total']=autonomous_coms_wide_df.sum(axis=1)\n",
    "#autonomous_coms_wide_df\n",
    "\n",
    "routes_df2=routes_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "routes_wide_df=pd.pivot(routes_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = routes_df['Descripcion'].unique()\n",
    "routes_wide_df=routes_wide_df[cols]\n",
    "routes_wide_df.fillna(0,inplace=True)\n",
    "routes_wide_df['Total']=routes_wide_df.sum(axis=1)\n",
    "#routes_wide_df\n",
    "\n",
    "continents_df2=continents_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "continents_wide_df=pd.pivot(continents_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = continents_df['Descripcion'].unique()\n",
    "continents_wide_df=continents_wide_df[cols]\n",
    "continents_wide_df.fillna(0,inplace=True)\n",
    "continents_wide_df['Total']=continents_wide_df.sum(axis=1)\n",
    "#continents_wide_df\n",
    "\n",
    "ages_df2=ages_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "ages_wide_df=pd.pivot(ages_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = ages_df['Descripcion'].unique()\n",
    "ages_wide_df=ages_wide_df[cols]\n",
    "ages_wide_df.fillna(0,inplace=True)\n",
    "ages_wide_df['Total']=ages_wide_df.sum(axis=1)\n",
    "#ages_wide_df\n",
    "\n",
    "groups_df2=groups_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "groups_wide_df=pd.pivot(groups_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = groups_df['Descripcion'].unique()\n",
    "groups_wide_df=groups_wide_df[cols]\n",
    "groups_wide_df.fillna(0,inplace=True)\n",
    "groups_wide_df['Total']=groups_wide_df.sum(axis=1)\n",
    "#groups_wide_df\n",
    "\n",
    "transportation_df2=transportation_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "transportation_wide_df=pd.pivot(transportation_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = transportation_df['Descripcion'].unique()\n",
    "transportation_wide_df=transportation_wide_df[cols]\n",
    "transportation_wide_df.fillna(0,inplace=True)\n",
    "transportation_wide_df['Total']=transportation_wide_df.sum(axis=1)\n",
    "#transportation_wide_df\n",
    "\n",
    "motives_df2=motives_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "motives_wide_df=pd.pivot(motives_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = motives_df['Descripcion'].unique()\n",
    "motives_wide_df=motives_wide_df[cols]\n",
    "motives_wide_df.fillna(0,inplace=True)\n",
    "motives_wide_df['Total']=motives_wide_df.sum(axis=1)\n",
    "#motives_wide_df\n",
    "\n",
    "countries_df2=countries_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "countries_wide_df=pd.pivot(countries_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = countries_df['Descripcion'].unique()\n",
    "countries_wide_df=countries_wide_df[cols]\n",
    "countries_wide_df.fillna(0,inplace=True)\n",
    "countries_wide_df['Total']=countries_wide_df.sum(axis=1)\n",
    "#countries_wide_df\n",
    "\n",
    "origin_df2=origin_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "origin_wide_df=pd.pivot(origin_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = origin_df['Descripcion'].unique()\n",
    "origin_wide_df=origin_wide_df[cols]\n",
    "origin_wide_df.fillna(0,inplace=True)\n",
    "origin_wide_df['Total']=origin_wide_df.sum(axis=1)\n",
    "#origin_wide_df\n",
    "\n",
    "gender_df2=gender_df.groupby(['ID','Anho','Mes','Descripcion'], as_index =False)['Total'].sum()\n",
    "gender_wide_df=pd.pivot(gender_df2,index=['ID','Anho','Mes'],columns='Descripcion',values='Total')\n",
    "cols = gender_df['Descripcion'].unique()\n",
    "gender_wide_df=gender_wide_df[cols]\n",
    "gender_wide_df.fillna(0,inplace=True)\n",
    "gender_wide_df['Total']=gender_wide_df.sum(axis=1)\n",
    "gender_wide_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 8:__ Output Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path to use to save the dataframes to Excel files  ###\n",
    "save_path_xl=\"C:/Sites/CaminoAwait/Data/Excel_Wide/\"\n",
    "\n",
    "totals_df.to_excel(save_path_xl+\"camino_totals_wide.xlsx\", engine=\"openpyxl\")\n",
    "autonomous_coms_wide_df.to_excel(save_path_xl+\"camino_autonomous_comm_wide.xlsx\", engine=\"openpyxl\")\n",
    "routes_wide_df.to_excel(save_path_xl+\"camino_routes_wide.xlsx\", engine=\"openpyxl\")\n",
    "continents_wide_df.to_excel(save_path_xl+\"camino_continents_wide.xlsx\", engine=\"openpyxl\")\n",
    "ages_wide_df.to_excel(save_path_xl+\"camino_ages_wide.xlsx\", engine=\"openpyxl\")\n",
    "groups_wide_df.to_excel(save_path_xl+\"camino_groups_wide.xlsx\", engine=\"openpyxl\")\n",
    "transportation_wide_df.to_excel(save_path_xl+\"camino_transportation_wide.xlsx\", engine=\"openpyxl\")\n",
    "motives_wide_df.to_excel(save_path_xl+\"camino_motives_wide.xlsx\", engine=\"openpyxl\")\n",
    "countries_wide_df.to_excel(save_path_xl+\"camino_countries_wide.xlsx\", engine=\"openpyxl\")\n",
    "origin_wide_df.to_excel(save_path_xl+\"camino_origin_wide.xlsx\", engine=\"openpyxl\")\n",
    "gender_wide_df.to_excel(save_path_xl+\"camino_gender_wide.xlsx\", engine=\"openpyxl\")\n",
    "totals_true_df.to_excel(save_path_xl+\"camino_totals_true_wide.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "### Path to use to save the dataframes to Excel files as Long Data ###\n",
    "save_path_xl_long=\"C:/Sites/CaminoAwait/Data/Excel_Long/\"\n",
    "\n",
    "totals_df.to_excel(save_path_xl_long+\"camino_totals_long.xlsx\", engine=\"openpyxl\")\n",
    "autonomous_coms_df2.to_excel(save_path_xl_long+\"camino_autonomous_comm_long.xlsx\", engine=\"openpyxl\")\n",
    "routes_df2.to_excel(save_path_xl_long+\"camino_routes_long.xlsx\", engine=\"openpyxl\")\n",
    "continents_df2.to_excel(save_path_xl_long+\"camino_continents_long.xlsx\", engine=\"openpyxl\")\n",
    "ages_df2.to_excel(save_path_xl_long+\"camino_ages_long.xlsx\", engine=\"openpyxl\")\n",
    "groups_df2.to_excel(save_path_xl_long+\"camino_groups_long.xlsx\", engine=\"openpyxl\")\n",
    "transportation_df2.to_excel(save_path_xl_long+\"camino_transportation_long.xlsx\", engine=\"openpyxl\")\n",
    "motives_df2.to_excel(save_path_xl_long+\"camino_motives_long.xlsx\", engine=\"openpyxl\")\n",
    "countries_df2.to_excel(save_path_xl_long+\"camino_countries_long.xlsx\", engine=\"openpyxl\")\n",
    "origin_df2.to_excel(save_path_xl_long+\"camino_origin_long.xlsx\", engine=\"openpyxl\")\n",
    "gender_df2.to_excel(save_path_xl_long+\"camino_gender_long.xlsx\", engine=\"openpyxl\")\n",
    "totals_true_df.to_excel(save_path_xl_long+\"camino_totals_true_long.xlsx\", engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 9:__ Output CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path to use to save the dataframes to CSV files  ###\n",
    "save_path_csv=\"C:/Sites/CaminoAwait/Data/CSV_Wide/\"\n",
    "\n",
    "\n",
    "totals_df.to_csv(save_path_csv+\"camino_totals_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "autonomous_coms_wide_df.to_csv(save_path_csv+\"camino_autonomous_comm_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "routes_wide_df.to_csv(save_path_csv+\"camino_routes_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "continents_wide_df.to_csv(save_path_csv+\"camino_continents_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "ages_wide_df.to_csv(save_path_csv+\"camino_ages_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "groups_wide_df.to_csv(save_path_csv+\"camino_groups_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "transportation_wide_df.to_csv(save_path_csv+\"camino_transportation_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "motives_wide_df.to_csv(save_path_csv+\"camino_motives_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "countries_wide_df.to_csv(save_path_csv+\"camino_countries_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "origin_wide_df.to_csv(save_path_csv+\"camino_origin_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "gender_wide_df.to_csv(save_path_csv+\"camino_gender_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "totals_true_df.to_csv(save_path_csv+\"camino_totals_true_wide.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "\n",
    "### Path to use to save the dataframes to CSV files as Long Data ###\n",
    "save_path_csv_long=\"C:/Sites/CaminoAwait/Data/CSV_Long/\"\n",
    "\n",
    "totals_df.to_csv(save_path_csv_long+\"camino_totals_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "autonomous_coms_df2.to_csv(save_path_csv_long+\"camino_autonomous_comm_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "routes_df2.to_csv(save_path_csv_long+\"camino_routes_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "continents_df2.to_csv(save_path_csv_long+\"camino_continents_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "ages_df2.to_csv(save_path_csv_long+\"camino_ages_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "groups_df2.to_csv(save_path_csv_long+\"camino_groups_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "transportation_df2.to_csv(save_path_csv_long+\"camino_transportation_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "motives_df2.to_csv(save_path_csv_long+\"camino_motives_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "countries_df2.to_csv(save_path_csv_long+\"camino_countries_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "origin_df2.to_csv(save_path_csv_long+\"camino_origin_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "gender_df2.to_csv(save_path_csv_long+\"camino_gender_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "totals_true_df.to_csv(save_path_csv_long+\"camino_totals_true_long.csv\",  encoding=\"cp1252\",header=True, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Additional Tables:__ File with cumulative pilgrims per point of origin.\n",
    "#### __Step 10:__ Load Additional library to model flow with a directed acyclical graph (DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### __Step 11:__ Read in Origins file in long format and route network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path_csv_long=\"C:/Sites/CaminoAwait/Data/CSV_Long/\"\n",
    "camino_origins=pd.read_csv(read_path_csv_long+\"camino_origin_long.csv\",  header=0,index_col=0,encoding=\"cp1252\",dtype={'ID':int,'Total':int})\n",
    "camino_origins.rename(columns = {'Descripcion':'Origen'}, inplace = True)\n",
    "read_path_csv_network=\"C:/Sites/\"\n",
    "camino_network=pd.read_csv(read_path_csv_network+\"RoutesLatLng02.csv\",  header=0,encoding=\"cp1252\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 12:__ Read in Origins file in long format and route network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_ex = nx.DiGraph()\n",
    "G_ex.add_nodes_from([0],node_type=\"END\",node_color=\"r\")\n",
    "\n",
    "for idx,row in camino_network.iterrows():\n",
    "    ##print(row)\n",
    "    ##print(camino_network[:nextid])\n",
    "    print(\"IDX:\",row[\"ID\"],\"Next:\",row[\"NextID\"])\n",
    "    ##if row[\"NextID\"]>=0: \n",
    "        \n",
    "    if row[\"NextID\"]>=0: \n",
    "        G_ex.add_nodes_from([row[\"ID\"]],node_type=\"WAY\",node_color=\"b\")\n",
    "\n",
    "for idx,row in camino_network.iterrows():\n",
    "    ##print(row)\n",
    "    ##print(camino_network[:nextid])\n",
    "    ##if row[\"NextID\"]>=0: print(\"IDX:\",row[\"ID\"],\"Next:\",row[\"NextID\"])\n",
    "    if row[\"NextID\"]>=0: \n",
    "        G_ex.add_edges_from([(row[\"ID\"],row[\"NextID\"])])\n",
    "    ##G_ex.add_nodes_from(row[\"ID\"],node_type=\"WAY\",node_color=\"b\")   \n",
    "nx.draw(G_ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 13:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in list(range(0,len(camino_network.index)))]\n",
    "df_node_to_sum=pd.DataFrame()\n",
    "ID_Node=[]\n",
    "Node_Ancestors=[]\n",
    "for i in list(range(0,len(camino_network.index))):\n",
    "    ##print(\"i:\",i,\":\",sorted(nx.ancestors(G_ex,i)|{i}))\n",
    "    ID_Node.append(i)\n",
    "    Node_Ancestors.append(sorted(nx.ancestors(G_ex,i)|{i}))\n",
    "\n",
    "df_node_to_sum=pd.DataFrame(list(zip(ID_Node,Node_Ancestors)),columns=['ID_Node','Node_Ancestors'])\n",
    "##print(len(ID_Node))\n",
    "##print(len(Node_Ancestors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 14:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflookup=camino_network[['Origen','ID']].copy()\n",
    "print(dflookup)\n",
    "camino_origins2=camino_origins.merge(right=dflookup,left_on=\"Origen\",right_on=\"Origen\",suffixes=('','_Node'))\n",
    "print(\"Org0\",camino_origins)\n",
    "print(\"Org2\",camino_origins2)\n",
    "camino_origins2=camino_origins2.sort_values(by=[\"ID\"])\n",
    "\n",
    "camino_origins2=camino_origins.merge(how=\"left\",right=dflookup,left_on=\"Origen\",right_on=\"Origen\",suffixes=('','_Node'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 15:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilgrim_sum_node_yr_month=[]\n",
    "for node_idx in list(range(0,len(camino_origins2))):\n",
    "    #print(\"node_yr:\",camino_origins2.iloc[node_idx][\"Anho\"],\" node_month:\",camino_origins2.iloc[node_idx][\"Mes\"],\" ID_Node:\",camino_origins2.iloc[node_idx][\"ID_Node\"])\n",
    "    ##print(\"node_idx:\",node_idx,\" ID_Node>:\",df_node_to_sum.iloc[camino_origins2.iloc[node_idx][\"ID_Node\"]],\" Ancestors>:\",df_node_to_sum.iloc[camino_origins2.iloc[node_idx][\"ID_Node\"]][\"Node_Ancestors\"])\n",
    "    \n",
    "    ##print(\"node_idx:\",node_idx,\" ID_Node:\",int(camino_origins2.iloc[node_idx][\"ID_Node\"]))\n",
    "    ##print(\"node_idx:\",node_idx,\"node_yr:\",camino_origins2.iloc[node_idx][\"Anho\"],\" node_month:\",camino_origins2.iloc[node_idx][\"Mes\"],\" ID_Node:\",camino_origins2.iloc[node_idx][\"ID_Node\"],\" Ancestors>:\",df_node_to_sum.iloc[int(camino_origins2.iloc[node_idx][\"ID_Node\"])][\"Node_Ancestors\"])\n",
    "    \n",
    "    ##print\n",
    "    pilgrim_sum_node_yr_month.append(\n",
    "    camino_origins2[\n",
    "    (camino_origins2[\"Anho\"]==camino_origins2.iloc[node_idx][\"Anho\"])&\n",
    "    (camino_origins2[\"Mes\"]== camino_origins2.iloc[node_idx][\"Mes\"] )&\n",
    "    (camino_origins2[\"ID_Node\"].isin(df_node_to_sum.iloc[camino_origins2.iloc[node_idx][\"ID_Node\"]][\"Node_Ancestors\"]))\n",
    "    ][\"Total\"].sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 16:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_proto(camino_network_in):\n",
    "    proto_camino_network_out=camino_network_in[['ID','Origen']].copy()\n",
    "    proto_camino_network_out.rename(columns={'ID':'ID_Node'},inplace=True)\n",
    "    return proto_camino_network_out\n",
    "\n",
    "#num_to_repeat=pd.unique(camino_origins['ID'])\n",
    "\n",
    "proto_camino_network=make_proto(camino_network)\n",
    "print(\"proto:\",proto_camino_network)\n",
    "\n",
    "num_to_repeat=len(camino_network)\n",
    "print(\"num_to_repeat:\",num_to_repeat)\n",
    "camino_origins_all=pd.DataFrame()\n",
    "\n",
    "for i in pd.unique(camino_origins['ID']):\n",
    "    print(int(str(i)[0:4]),\" \",int(str(i)[4:]))\n",
    "    list_years=[int(str(i)[0:4])]*num_to_repeat\n",
    "    list_mnths=[int(str(i)[4:])]*num_to_repeat\n",
    "    temp_camino_network=proto_camino_network.copy()\n",
    "    #temp_camino_network.rename(columns={'ID':'ID_Node'},inplace=True)\n",
    "    temp_camino_network[\"ID\"]=i\n",
    "    temp_camino_network[\"Anho\"]=list_years\n",
    "    temp_camino_network[\"Mes\"]=list_mnths\n",
    "    \n",
    "    \n",
    "    temp_camino_network=pd.merge(temp_camino_network,camino_origins[[\"ID\",\"Origen\",\"Total\"]],how=\"left\",left_on=[\"ID\",\"Origen\"],right_on=[\"ID\",\"Origen\"],suffixes=[\"\",\"_drop\"])\n",
    "    temp_camino_network[\"Total\"]=temp_camino_network[\"Total\"].fillna(0)\n",
    "\n",
    "\n",
    "    pilgrim_sum_node_yr_month=[]\n",
    "    for node_idx in list(range(0,len(temp_camino_network))):\n",
    "    \n",
    "        pilgrim_sum_node_yr_month.append(\n",
    "        temp_camino_network[\n",
    "        (temp_camino_network[\"Anho\"]==temp_camino_network.iloc[node_idx][\"Anho\"])&\n",
    "        (temp_camino_network[\"Mes\"]== temp_camino_network.iloc[node_idx][\"Mes\"] )&\n",
    "        (temp_camino_network[\"ID_Node\"].isin(df_node_to_sum.iloc[temp_camino_network.iloc[node_idx][\"ID_Node\"]][\"Node_Ancestors\"]))\n",
    "        ][\"Total\"].sum()\n",
    "        )\n",
    "    temp_camino_network[\"Total_Origen_Incoming\"]=pilgrim_sum_node_yr_month-temp_camino_network[\"Total\"]\n",
    "    temp_camino_network[\"Total_Origin_Cumulative\"]=pilgrim_sum_node_yr_month\n",
    "\n",
    "    print(\"Temp\") \n",
    "    print(temp_camino_network)\n",
    "    #print(temp_camino_network)\n",
    "    camino_origins_all=pd.concat([camino_origins_all,temp_camino_network],axis=0)\n",
    "camino_origins_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Step 17:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_csv_long=\"C:/Sites/CaminoAwait/Data/CSV_Long/\"\n",
    "camino_origins_all.to_csv(save_path_csv_long+\"camino_origins_cumulative_long_new.csv\",  encoding=\"cp1252\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __END__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c585f91d3623973be3accc48b0d5e967ce904a396a0f0c8bda7b100d8b60333f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
